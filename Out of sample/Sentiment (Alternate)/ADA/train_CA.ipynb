{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff882a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Callable\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve, auc, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "import vectorbt as vbt\n",
    "from scipy.optimize import minimize\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651f05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_BTC = False\n",
    "PAIR = 'ADAUSDT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b8425",
   "metadata": {},
   "source": [
    "## TBL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28827d6",
   "metadata": {},
   "source": [
    "Label the price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b83c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labeler:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Initialize the labeler.\n",
    "\n",
    "        Args:\n",
    "        name (str): The name of the labeler.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the labeler to the data.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual fitting logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to fit the labeler to.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Transform the data into labels.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual transformation logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to transform into labels.\n",
    "\n",
    "        Returns:\n",
    "        any: The labels.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "class TripleBarrierLabeler(Labeler):\n",
    "    def __init__(self, volatility_period=7, upper_barrier_factor=1, lower_barrier_factor=1, vertical_barrier=7, min_trend_days=2, barrier_type='volatility', touch_type=\"HL\", up_label=2, neutral_label=1, down_label=0):\n",
    "        \"\"\"\n",
    "        Initialize the labeler.\n",
    "        \"\"\"\n",
    "        super().__init__(name=\"triple barrier labeling\")\n",
    "        self.volatility_period = volatility_period\n",
    "        self.upper_barrier_factor = upper_barrier_factor\n",
    "        self.lower_barrier_factor = lower_barrier_factor\n",
    "        self.vertical_barrier = vertical_barrier\n",
    "        self.min_trend_days = min_trend_days\n",
    "        self.barrier_type = barrier_type\n",
    "        self.touch_type = touch_type\n",
    "        self.up_label = up_label\n",
    "        self.down_label = down_label\n",
    "        self.neutral_label = neutral_label\n",
    "\n",
    "    def calculate_barriers(self, df, i, window):\n",
    "        \"\"\"calculate the barriers based on either volatility or returns of the backward window\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Data\n",
    "            i (pd.index): the index of the beginning of the window\n",
    "            window (int): window size\n",
    "\n",
    "        Returns:\n",
    "            df: Data including barriers for the forward window\n",
    "        \"\"\"\n",
    "        end_window = min(i+window, len(df)-1)  # Ensure the window does not exceed the dataframe\n",
    "\n",
    "        # Calculate the mean volatility or daily returns over the volatility_period\n",
    "        if self.barrier_type == 'volatility':\n",
    "            current_value = df.loc[i, 'volatility']\n",
    "        elif self.barrier_type == 'returns':\n",
    "            current_value = df.loc[i, 'daily_returns']\n",
    "        else:\n",
    "            raise ValueError(\"Invalid barrier_type. Choose either 'volatility' or 'returns'\")\n",
    "\n",
    "        df.loc[i:end_window, 'upper_barrier'] = df.loc[i, 'close'] + (df.loc[i, 'close'] * current_value * self.upper_barrier_factor)\n",
    "        df.loc[i:end_window, 'lower_barrier'] = df.loc[i, 'close'] - (df.loc[i, 'close'] * current_value * self.lower_barrier_factor)\n",
    "        return df\n",
    "\n",
    "    def label_observations(self, df, origin, i, label):\n",
    "        df.loc[origin:i+1, 'label'] = label\n",
    "        return df\n",
    "\n",
    "    def get_daily_vol(self, close, span=30):\n",
    "        \"\"\"\n",
    "        Calculate the EWMA volatility of closing prices.\n",
    "        \n",
    "        Parameters:\n",
    "        - close: A pandas Series of closing prices.\n",
    "        - span: The span for the EWMA standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "        - A pandas Series of EWMA volatility estimates.\n",
    "        \"\"\"\n",
    "        # Calculate log returns\n",
    "        log_returns = np.log(close / close.shift(1))\n",
    "        \n",
    "        # Calculate EWMA volatility\n",
    "        ewma_volatility = log_returns.ewm(span=span).std()\n",
    "        \n",
    "        return log_returns, ewma_volatility\n",
    "\n",
    "    def fit(self, sdf):\n",
    "        df = sdf.copy()\n",
    "        # Calculate daily returns and volatility\n",
    "        df['daily_returns'], df['volatility'] = self.get_daily_vol(df.close, self.volatility_period)\n",
    "\n",
    "        df = df.reset_index()\n",
    "        # Initialize label and window start\n",
    "        df['label'] = self.neutral_label\n",
    "        df['window_start'] = False\n",
    "\n",
    "        self.data = df\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transform the data into labels.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The labels.\n",
    "        \"\"\"\n",
    "        window = self.vertical_barrier\n",
    "        origin = 0\n",
    "        touch_upper = lambda high, barrier: high >= barrier\n",
    "        touch_lower = lambda low, barrier: low <= barrier\n",
    "        # For each observation\n",
    "        for i in range(0, len(self.data)):\n",
    "            # Define your barriers at the beginning of each window\n",
    "            if i == origin:\n",
    "                self.data = self.calculate_barriers(self.data, i, window)\n",
    "                self.data.loc[i, 'window_start'] = True  # Mark the start of the window\n",
    "\n",
    "            # one of the conditions were met\n",
    "            if touch_upper(self.data.loc[i, \"high\" if self.touch_type == 'HL' else 'close'], self.data.loc[i, \"upper_barrier\"]):\n",
    "                if (i - origin > self.min_trend_days):\n",
    "                    # label the observations\n",
    "                    self.data = self.label_observations(self.data, origin, i, self.up_label)\n",
    "                    # set new origin\n",
    "                    origin = i + 1 if i + 1 < len(self.data) else i  # Check if i + 1 is within the DataFrame's index\n",
    "                    # reset window\n",
    "                    window = self.vertical_barrier\n",
    "            elif touch_lower(self.data.loc[i, \"low\" if self.touch_type == 'HL' else 'close'], self.data.loc[i, \"lower_barrier\"]):\n",
    "                if (i - origin > self.min_trend_days):\n",
    "                    # label the observations\n",
    "                    self.data = self.label_observations(self.data, origin, i, self.down_label)\n",
    "                    # set new origin\n",
    "                    origin = i + 1 if i + 1 < len(self.data) else i  # Check if i + 1 is within the DataFrame's index\n",
    "                    # reset window\n",
    "                    window = self.vertical_barrier\n",
    "\n",
    "            # none of the conditions were met\n",
    "            else:\n",
    "                if window > 0:\n",
    "                    # reduce window size by one\n",
    "                    window = window - 1\n",
    "                else:\n",
    "                    # reset window\n",
    "                    window = self.vertical_barrier\n",
    "                    # label neutral from origin to origin + window\n",
    "                    self.data.loc[origin:min(origin+window, len(self.data)-1), 'label'] = self.neutral_label  # Ensure the window does not exceed the dataframe\n",
    "                    # set origin to the next id\n",
    "                    origin = i + 1 if i + 1 < len(self.data) else i  # Check if i + 1 is within the DataFrame's index\n",
    "\n",
    "        self.data = self.data.set_index(\"timestamp\")\n",
    "        return self.data\n",
    "\n",
    "def optimize_parameters(price_df, param_grid, initial_capital=100000, num_starts=10, optimization_interval='6M'):\n",
    "    if optimization_interval is None:\n",
    "        # Use the whole dataset for optimization\n",
    "        best_params, best_sharpe_ratio = tune_hyperparameters_scipy(price_df, param_grid, initial_capital, num_starts)\n",
    "        return pd.DataFrame([{\n",
    "            'start': price_df.index[0],\n",
    "            'end': price_df.index[-1],\n",
    "            'params': best_params,\n",
    "            'sharpe_ratio': best_sharpe_ratio,\n",
    "        }])\n",
    "    else:\n",
    "        # Optimize parameters at specified intervals\n",
    "        intervals = pd.date_range(start=price_df.index[0], end=price_df.index[-1], freq=optimization_interval)\n",
    "        optimized_params = []\n",
    "\n",
    "        for start, end in zip(intervals[:-1], intervals[1:]):\n",
    "            interval_df = price_df.loc[start:end]\n",
    "            best_params, best_sharpe_ratio = tune_hyperparameters_scipy(interval_df, param_grid, initial_capital, num_starts)\n",
    "            optimized_params.append({\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'params': best_params,\n",
    "                'sharpe_ratio': best_sharpe_ratio,\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(optimized_params)\n",
    "\n",
    "def tune_hyperparameters_scipy(price_df, param_grid, initial_capital=100000, num_starts=10):\n",
    "    best_sharpe_ratio, best_net_profit, best_params = -np.inf, -np.inf, None\n",
    "\n",
    "    param_space = [\n",
    "        param_grid['volatility_period'],\n",
    "        param_grid['upper_barrier_factor'],\n",
    "        param_grid['lower_barrier_factor'],\n",
    "        param_grid['vertical_barrier'],\n",
    "        param_grid['barrier_type'],\n",
    "        param_grid['touch_type']\n",
    "    ]\n",
    "\n",
    "    bounds = [(0, len(space) - 1) for space in param_space]\n",
    "\n",
    "    def bounds_to_params(x):\n",
    "        return {key: space[int(idx)] for key, space, idx in zip(param_grid.keys(), param_space, x)}\n",
    "\n",
    "    def objective_wrapper(x):\n",
    "        params = bounds_to_params(x)\n",
    "        return sharpe_ratio_objective(params, price_df, initial_capital)\n",
    "\n",
    "    for _ in tqdm(range(num_starts), desc=\"Optimizing\"):\n",
    "        initial_guess = [np.random.randint(len(space)) for space in param_space]\n",
    "        result = minimize(objective_wrapper, initial_guess, method='SLSQP', bounds=bounds)\n",
    "\n",
    "        if result.success and -result.fun > best_sharpe_ratio:\n",
    "            best_sharpe_ratio = -result.fun\n",
    "            best_params = bounds_to_params(result.x)\n",
    "\n",
    "    return best_params, best_sharpe_ratio\n",
    "\n",
    "def sharpe_ratio_objective(params, price_df, initial_capital):\n",
    "    labeler = TripleBarrierLabeler(**params)\n",
    "    labeler.fit(price_df)\n",
    "    labeled_df = labeler.transform()\n",
    "    labeled_df.set_index(price_df.index, inplace=True)\n",
    "    sharpe_ratio = strategize_and_measure(labeled_df, init_capital=initial_capital)\n",
    "    return -sharpe_ratio  # Minimize negative Sharpe Ratio to maximize Sharpe Ratio\n",
    "\n",
    "def strategize_and_measure(price_df, init_capital=1000, fee=0.006):\n",
    "    entries = price_df[\"label\"] > 1\n",
    "    short_entries = price_df[\"label\"] < 1\n",
    "    # Calculate the percentage changes for TP and SL\n",
    "    price_df['tp_stop'] = (price_df['upper_barrier'] - price_df['close']) / price_df['close']\n",
    "    price_df['sl_stop'] = (price_df['close'] - price_df['lower_barrier']) / price_df['close']\n",
    "    # Replace negative values with 0\n",
    "    price_df['tp_stop'] = price_df['tp_stop'].apply(lambda x: max(x, 0))\n",
    "    price_df['sl_stop'] = price_df['sl_stop'].apply(lambda x: max(x, 0))\n",
    "\n",
    "    # Run the backtest\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=price_df['close'],\n",
    "        entries=entries,\n",
    "        short_entries=short_entries,\n",
    "        # size=size,\n",
    "        tp_stop=price_df['tp_stop'],\n",
    "        sl_stop=price_df['sl_stop'],\n",
    "        init_cash=1000,  # Initial cash value\n",
    "        fees=fee,  # Commission fees (0.5%)\n",
    "        freq=\"D\"\n",
    "    )\n",
    "\n",
    "    return pf.sharpe_ratio()\n",
    "\n",
    "def compute_metrics_classification(labels, preds, probs, metrics_to_return=None, average=\"macro\", multi_class=\"ovr\"):\n",
    "    \"\"\"\n",
    "    Compute classification metrics based on the model's predictions and the true labels.\n",
    "\n",
    "    Args:\n",
    "    labels (any): The true labels.\n",
    "    preds (any): The model's predictions.\n",
    "    probs (any): The model's probabilities\n",
    "    metrics_to_return (list): List of metric names to compute and return.\n",
    "\n",
    "    Returns:\n",
    "    dict: The computed classification metrics.\n",
    "    \"\"\"\n",
    "    if metrics_to_return is None:\n",
    "        metrics_to_return = [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_score\", \"confusion_matrix\"]\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if \"precision\" in metrics_to_return or \"recall\" in metrics_to_return or \"f1\" in metrics_to_return:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=average)\n",
    "        if \"precision\" in metrics_to_return:\n",
    "            metrics[\"precision\"] = precision\n",
    "        if \"recall\" in metrics_to_return:\n",
    "            metrics[\"recall\"] = recall\n",
    "        if \"f1\" in metrics_to_return:\n",
    "            metrics[\"f1\"] = f1\n",
    "\n",
    "    if \"accuracy\" in metrics_to_return:\n",
    "        metrics[\"accuracy\"] = accuracy_score(labels, preds)\n",
    "\n",
    "    if \"roc_score\" in metrics_to_return:\n",
    "        metrics[\"roc_score\"] = roc_auc_score(labels, probs, multi_class=multi_class)\n",
    "\n",
    "    if \"confusion_matrix\" in metrics_to_return:\n",
    "        metrics[\"confusion_matrix\"] = confusion_matrix(labels, preds)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def pandas_data_loader(addr: str, columns: List[str], *transforms: Callable[[pd.DataFrame], pd.DataFrame]) -> pd.DataFrame:\n",
    "    # Load the data from the CSV file\n",
    "    df = pd.read_csv(addr, usecols=columns)\n",
    "\n",
    "    # Apply each transform to the DataFrame\n",
    "    for transform in transforms:\n",
    "        df = transform(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Transform index to datetime\n",
    "def index_to_datetime(df, unit=\"s\"):\n",
    "    df.index = pd.to_datetime(df.index, unit=unit)\n",
    "    return df\n",
    "# Transform col to index\n",
    "to_index = lambda col, df: df.set_index(col)\n",
    "# Rename text_plit to text\n",
    "rename = lambda original, new, df: df.rename(columns={original: new})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dcdb8f",
   "metadata": {},
   "source": [
    "Load the price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080f4c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title load price data\n",
    "address = f\"{PAIR}_1d_2022.csv\" # @param {\"type\":\"string\",\"placeholder\":\"./raw/daily-2020.csv\"}\n",
    "# Loading the price data\n",
    "columns = [\"timestamp\", \"close\", \"open\", \"high\", \"low\", \"volume\"]\n",
    "price_df = pandas_data_loader(address, columns, partial(to_index, \"timestamp\"), partial(index_to_datetime, unit=None))\n",
    "price_df = price_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd1eaa",
   "metadata": {},
   "source": [
    "Define labelling hyper-parameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693f25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    # 'volatility_period': [8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    'volatility_period': [7], # As per the original code, we use 7 days for volatility\n",
    "    'upper_barrier_factor': [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "    'lower_barrier_factor': [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "    'vertical_barrier': [8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    'barrier_type': ['volatility'],\n",
    "    'touch_type': ['HL']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccf306",
   "metadata": {},
   "source": [
    "Label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b139a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3484/2717499963.py:179: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  intervals = pd.date_range(start=price_df.index[0], end=price_df.index[-1], freq=optimization_interval)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691bb15095c648f4917ac27e6cb821ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimizing:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    75\n",
      "2    67\n",
      "1    40\n",
      "Name: count, dtype: int64 0    5.847579\n",
      "Name: sharpe_ratio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Optimize parameters every six months\n",
    "optimized_params_df = optimize_parameters(price_df, param_grid, initial_capital=100000, num_starts=500, optimization_interval='6M')\n",
    "\n",
    "# Apply the optimized parameters to label the data\n",
    "labeled_df = pd.DataFrame()\n",
    "for _, row in optimized_params_df.iterrows():\n",
    "    start, end, params = row['start'], row['end'], row['params']\n",
    "    interval_df = price_df.loc[start:end]\n",
    "    labeler = TripleBarrierLabeler(**params)\n",
    "    labeler.fit(interval_df)\n",
    "    labeled_interval_df = labeler.transform()\n",
    "    labeled_df = pd.concat([labeled_df, labeled_interval_df])\n",
    "\n",
    "# Check the label distribution\n",
    "print(labeled_df.label.value_counts(), optimized_params_df.sharpe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27253e6",
   "metadata": {},
   "source": [
    "Save the labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cea3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.to_csv(\"optimized_labeled_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a735e",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85b2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from typing import List, Callable\n",
    "from functools import partial\n",
    "import warnings\n",
    "import gdown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, ClassLabel\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from datasets import Dataset as HuggingfaceDataset\n",
    "from torch.utils.data import DataLoader, Dataset as torchDS\n",
    "from functools import partial\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import time\n",
    "import neptune\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
    "from transformers.integrations import NeptuneCallback\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18db6b",
   "metadata": {},
   "source": [
    "Disable warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326c51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"accelerate\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"sklearn.metrics._classification\")\n",
    "neptune_logger = logging.getLogger('neptune')\n",
    "neptune_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0d365",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590f41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Predifined Classes\n",
    "class Model:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "\n",
    "        Args:\n",
    "        name (str): The name of the model.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        \"\"\"\n",
    "        Train the model on the given data and labels.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual training logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to train the model on.\n",
    "        labels (any): The labels for the data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Use the model to make predictions on the given data.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual prediction logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to make predictions on.\n",
    "\n",
    "        Returns:\n",
    "        any: The predictions.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def evaluate(self, data, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given data and labels.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual evaluation logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to evaluate the model on.\n",
    "        labels (any): The labels for the data.\n",
    "\n",
    "        Returns:\n",
    "        any: The evaluation results.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def compute_metrics(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Compute metrics based on the model's predictions and the true labels.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual metrics computation logic.\n",
    "\n",
    "        Args:\n",
    "        predictions (any): The model's predictions.\n",
    "        labels (any): The true labels.\n",
    "\n",
    "        Returns:\n",
    "        any: The computed metrics.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "class CryptoBERT(Model):\n",
    "    def __init__(self, model_addr=\"ElKulako/cryptobert\", save_path=f'./artifact/fine_tuned_model.pth', load_path=None, load_state_dict=False, input_task='classification'):\n",
    "        super().__init__(model_addr)\n",
    "        self.model_addr = model_addr\n",
    "        self.save_path = save_path\n",
    "        self.load_path = load_path\n",
    "        self.input_task = input_task\n",
    "        self.metrics = {}  # Initialize the metrics dictionary\n",
    "        self.labels = {}  # Initialize the labels dictionary\n",
    "        self.preds = {}  # Initialize the predictions dictionary\n",
    "        self.probs = {}  # Initialize the probabilities dictionary\n",
    "        self.eval_labels = {}\n",
    "        self.eval_preds = {}\n",
    "        self.eval_probs = {}\n",
    "        # Load configuration\n",
    "        config = AutoConfig.from_pretrained(model_addr)\n",
    "\n",
    "        # Adjust configuration for regression task\n",
    "        if input_task == \"regression\":\n",
    "            config.num_labels = 1  # Adjust for regression task\n",
    "        elif input_task == \"classification\":\n",
    "            config.num_labels = 3  # Adjust for classification task\n",
    "\n",
    "        # Load model with modified configuration\n",
    "        if load_state_dict:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_addr, config=config)\n",
    "            self.model.load_state_dict(torch.load(self.load_path))\n",
    "        else:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_addr, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "    def predict(self, data):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # Create the output directory if it doesn't exist\n",
    "        dir_name = os.path.dirname(path)\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        model_state = torch.load(path)\n",
    "        self.model.load_state_dict(model_state)\n",
    "        return self.model\n",
    "\n",
    "    def train(self, dataloader, device, optimizer, scheduler, learning_rate=2e-5, model_name=\"train\", neptune_run=None):\n",
    "        \"\"\"\n",
    "        Train the model on the given data and labels.\n",
    "\n",
    "        Args:\n",
    "        dataloader (DataLoader): The DataLoader for the training data.\n",
    "        device (torch.device): The device to train the model on.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        num_epochs (int): The number of epochs for training.\n",
    "        num_folds (int): The number of folds for cross-validation.\n",
    "        neptune_run (neptune.run.Run): The Neptune run instance.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[List, List, List, List]: The labels, predictions, probabilities, and losses for each batch.\n",
    "        \"\"\"\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_losses = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Progress...\", leave=False, dynamic_ncols=True):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Store labels, predictions and probabilities for metrics calculation\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            class_preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "            all_probs.append(preds.detach().cpu().numpy())  # Store probabilities\n",
    "            all_preds.append(class_preds.cpu().detach().numpy())\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "            all_losses.append(loss.item())\n",
    "\n",
    "            if neptune_run:\n",
    "                # Log metrics to Neptune\n",
    "                neptune_metrics = [\"accuracy\", \"precision\", \"f1\", \"recall\"]\n",
    "                # Compute metrics\n",
    "                metrics = self.compute_metrics_classification(np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_probs), neptune_metrics)\n",
    "                for metric_name in neptune_metrics:\n",
    "                    neptune_run[f\"{model_name}/{metric_name}\"].append(metrics.get(metric_name))\n",
    "                neptune_run[f\"{model_name}/loss\"].append(loss.item())\n",
    "\n",
    "        return all_labels, all_preds, all_probs, all_losses\n",
    "\n",
    "\n",
    "    def evaluate(self, dataloader, device, model_name=\"base\", neptune_run=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given data and labels.\n",
    "\n",
    "        Args:\n",
    "        dataloader (DataLoader): The DataLoader for the evaluation data.\n",
    "        device (torch.device): The device to evaluate the model on.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[List, List, List, list]: The labels, predictions, probabilities, losses for each batch.\n",
    "        \"\"\"\n",
    "        # Evaluation loop\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_losses = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating Progress...\", leave=False, dynamic_ncols=True):\n",
    "            with torch.no_grad():\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Get the predicted probabilities from the model's outputs\n",
    "                preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                # Convert the probabilities to class labels\n",
    "                class_preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "                all_probs.append(preds.cpu().numpy())  # Store probabilities\n",
    "                all_preds.append(class_preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_losses.append(loss.item())\n",
    "\n",
    "            if neptune_run:\n",
    "                # Log metrics to Neptune\n",
    "                neptune_metrics = [\"accuracy\", \"precision\", \"f1\", \"recall\"]\n",
    "                # Compute metrics\n",
    "                metrics = self.compute_metrics_classification(np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_probs), neptune_metrics)\n",
    "                for metric_name in neptune_metrics:\n",
    "                    neptune_run[f\"{model_name}/{metric_name}\"].append(metrics.get(metric_name))\n",
    "                neptune_run[f\"{model_name}/loss\"].append(loss.item())\n",
    "\n",
    "        return all_labels, all_preds, all_probs, all_losses\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics_classification(labels, preds, probs, metrics_to_return=None):\n",
    "        \"\"\"\n",
    "        Compute classification metrics based on the model's predictions and the true labels.\n",
    "\n",
    "        Args:\n",
    "        labels (any): The true labels.\n",
    "        preds (any): The model's predictions.\n",
    "        probs (any): The model's probabilities\n",
    "        metrics_to_return (list): List of metric names to compute and return.\n",
    "\n",
    "        Returns:\n",
    "        dict: The computed classification metrics.\n",
    "        \"\"\"\n",
    "        if metrics_to_return is None:\n",
    "            metrics_to_return = [\"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_score\", \"confusion_matrix\"]\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        if \"precision\" in metrics_to_return or \"recall\" in metrics_to_return or \"f1\" in metrics_to_return:\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "            if \"precision\" in metrics_to_return:\n",
    "                metrics[\"precision\"] = precision\n",
    "            if \"recall\" in metrics_to_return:\n",
    "                metrics[\"recall\"] = recall\n",
    "            if \"f1\" in metrics_to_return:\n",
    "                metrics[\"f1\"] = f1\n",
    "\n",
    "        if \"accuracy\" in metrics_to_return:\n",
    "            metrics[\"accuracy\"] = accuracy_score(labels, preds)\n",
    "\n",
    "        if \"roc_score\" in metrics_to_return:\n",
    "            metrics[\"roc_score\"] = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "\n",
    "        if \"confusion_matrix\" in metrics_to_return:\n",
    "            metrics[\"confusion_matrix\"] = confusion_matrix(labels, preds)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def compute_metrics_regression(labels, preds):\n",
    "        \"\"\"\n",
    "        Compute regression metrics based on the model's predictions and the true labels.\n",
    "\n",
    "        Args:\n",
    "        labels (any): The true labels.\n",
    "        preds (any): The model's predictions.\n",
    "\n",
    "        Returns:\n",
    "        dict: The computed regression metrics.\n",
    "        \"\"\"\n",
    "        mae = mean_absolute_error(labels, preds)\n",
    "        mse = mean_squared_error(labels, preds)\n",
    "\n",
    "        # Create a dictionary of metrics\n",
    "        metrics = {\n",
    "            \"mean_absolute_error\": mae,\n",
    "            \"mean_squared_error\": mse\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "        return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(path, labels, preds):\n",
    "        \"\"\"\n",
    "        Plot the confusion matrix for the given labels and predictions.\n",
    "\n",
    "        Args:\n",
    "        output_dir (str): The directory to save the confusion matrix plot.\n",
    "        labels (list): The true labels.\n",
    "        preds (list): The predicted labels.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Create the output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        conf_matrix = confusion_matrix(labels, preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Down', 'Neutral', 'Up'])\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curve(path, labels, probs):\n",
    "        \"\"\"\n",
    "        Plot the ROC curve for the given labels and probabilities.\n",
    "\n",
    "        Args:\n",
    "        path (str): The path to save the ROC curve plots.\n",
    "        labels (list): The true labels.\n",
    "        probs (list): The predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Create the output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        plt.figure()\n",
    "        # Binarize the labels for multi-class ROC AUC\n",
    "        labels = label_binarize(labels, classes=np.unique(labels))\n",
    "        n_classes = labels.shape[1]\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels[:, i], probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot all ROC curves\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(i, roc_auc[i]))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # Save the figure to the output directory with a unique name\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class Labeler:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Initialize the labeler.\n",
    "\n",
    "        Args:\n",
    "        name (str): The name of the labeler.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the labeler to the data.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual fitting logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to fit the labeler to.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Transform the data into labels.\n",
    "\n",
    "        This method should be overridden by subclasses to implement\n",
    "        the actual transformation logic.\n",
    "\n",
    "        Args:\n",
    "        data (any): The data to transform into labels.\n",
    "\n",
    "        Returns:\n",
    "        any: The labels.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "class TripleBarrierLabeler(Labeler):\n",
    "    def __init__(self, volatility_period=7, upper_barrier_factor=1, lower_barrier_factor=1, vertical_barrier=7, min_trend_days=2, barrier_type='volatility', touch_type=\"HL\", up_label=2, neutral_label=1, down_label=0):\n",
    "        \"\"\"\n",
    "        Initialize the labeler.\n",
    "        \"\"\"\n",
    "        super().__init__(name=\"triple barrier labeling\")\n",
    "        self.volatility_period = volatility_period\n",
    "        self.upper_barrier_factor = upper_barrier_factor\n",
    "        self.lower_barrier_factor = lower_barrier_factor\n",
    "        self.vertical_barrier = vertical_barrier\n",
    "        self.min_trend_days = min_trend_days\n",
    "        self.barrier_type = barrier_type\n",
    "        self.touch_type = touch_type\n",
    "        self.up_label = up_label\n",
    "        self.down_label = down_label\n",
    "        self.neutral_label = neutral_label\n",
    "\n",
    "    def calculate_barriers(self, df, i, window):\n",
    "        \"\"\"calculate the barriers based on either volatility or returns of the backward window\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Data\n",
    "            i (pd.index): the index of the beginning of the window\n",
    "            window (int): window size\n",
    "\n",
    "        Returns:\n",
    "            df: Data including barriers for the forward window\n",
    "        \"\"\"\n",
    "        end_window = min(i+window, len(df)-1)  # Ensure the window does not exceed the dataframe\n",
    "\n",
    "        # Calculate the mean volatility or daily returns over the volatility_period\n",
    "        if self.barrier_type == 'volatility':\n",
    "            current_value = df.loc[i, 'volatility']\n",
    "        elif self.barrier_type == 'returns':\n",
    "            current_value = df.loc[i, 'daily_returns']\n",
    "        else:\n",
    "            raise ValueError(\"Invalid barrier_type. Choose either 'volatility' or 'returns'\")\n",
    "\n",
    "        df.loc[i:end_window, 'upper_barrier'] = df.loc[i, 'close'] + (df.loc[i, 'close'] * current_value * self.upper_barrier_factor)\n",
    "        df.loc[i:end_window, 'lower_barrier'] = df.loc[i, 'close'] - (df.loc[i, 'close'] * current_value * self.lower_barrier_factor)\n",
    "        return df\n",
    "\n",
    "    def label_observations(self, df, origin, i, label):\n",
    "        df.loc[origin:i+1, 'label'] = label\n",
    "        return df\n",
    "\n",
    "    def get_daily_vol(self, close, span0=100):\n",
    "        \"\"\"\n",
    "        Calculate the daily volatility of closing prices.\n",
    "\n",
    "        Parameters:\n",
    "        - close: A pandas Series of closing prices.\n",
    "        - span0: The span for the EWM standard deviation.\n",
    "\n",
    "        Returns:\n",
    "        - A pandas Series of daily volatility estimates.\n",
    "        \"\"\"\n",
    "        # Find the start of the previous day for each day\n",
    "        prev_day_start = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
    "        prev_day_start = prev_day_start[prev_day_start > 0]\n",
    "\n",
    "        # Create a series with the start of the previous day for each day\n",
    "        prev_day_start = pd.Series(close.index[prev_day_start - 1], index=close.index[close.shape[0] - prev_day_start.shape[0]:])\n",
    "\n",
    "        # Calculate daily returns\n",
    "        daily_returns = close.loc[prev_day_start.index] / close.loc[prev_day_start.values].values - 1\n",
    "\n",
    "        # Calculate EWM standard deviation of daily returns\n",
    "        daily_vol = daily_returns.ewm(span=span0).std()\n",
    "\n",
    "        return daily_returns, daily_vol\n",
    "\n",
    "    def fit(self, sdf):\n",
    "        df = sdf.copy()\n",
    "        # Calculate daily returns and volatility\n",
    "        df['daily_returns'], df['volatility'] = self.get_daily_vol(df.close, self.volatility_period)\n",
    "\n",
    "        df = df.reset_index()\n",
    "        # Initialize label and window start\n",
    "        df['label'] = self.neutral_label\n",
    "        df['window_start'] = False\n",
    "\n",
    "        self.data = df\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transform the data into labels.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The labels.\n",
    "        \"\"\"\n",
    "        window = self.vertical_barrier\n",
    "        origin = 0\n",
    "        touch_upper = lambda high, barrier: high >= barrier\n",
    "        touch_lower = lambda low, barrier: low <= barrier\n",
    "        # For each observation\n",
    "        for i in range(0, len(self.data)):\n",
    "            # Define your barriers at the beginning of each window\n",
    "            if i == origin:\n",
    "                self.data = self.calculate_barriers(self.data, i, window)\n",
    "                self.data.loc[i, 'window_start'] = True  # Mark the start of the window\n",
    "\n",
    "            # one of the conditions were met\n",
    "            if touch_upper(self.data.loc[i, \"high\" if self.touch_type == 'HL' else 'close'], self.data.loc[i, \"upper_barrier\"]):\n",
    "                if (i - origin > self.min_trend_days):\n",
    "                    # label the observations\n",
    "                    self.data = self.label_observations(self.data, origin, i, self.up_label)\n",
    "                    # set new origin\n",
    "                    origin = i + 1 if i + 1 < len(self.data) else i  # Check if i + 1 is within the DataFrame's index\n",
    "                    # reset window\n",
    "                    window = self.vertical_barrier\n",
    "            elif touch_lower(self.data.loc[i, \"low\" if self.touch_type == 'HL' else 'close'], self.data.loc[i, \"lower_barrier\"]):\n",
    "                if (i - origin > self.min_trend_days):\n",
    "                    # label the observations\n",
    "                    self.data = self.label_observations(self.data, origin, i, self.down_label)\n",
    "                    # set new origin\n",
    "                    origin = i + 1 if i + 1 < len(self.data) else i  # Check if i + 1 is within the DataFrame's index\n",
    "                    # reset window\n",
    "                    window = self.vertical_barrier\n",
    "\n",
    "            # none of the conditions were met\n",
    "            else:\n",
    "                if window > 0:\n",
    "                    # reduce window size by one\n",
    "                    window = window - 1\n",
    "                else:\n",
    "                    # reset window\n",
    "                    window = self.vertical_barrier\n",
    "                    # label neutral from origin to origin + window\n",
    "                    self.data.loc[origin:min(origin+window, len(self.data)-1), 'label'] = self.neutral_label  # Ensure the window does not exceed the dataframe\n",
    "                    # set origin to the next id\n",
    "                    origin = i + 1 if i + 1 < len(self.data) else i  # Check if i + 1 is within the DataFrame's index\n",
    "\n",
    "        self.data = self.data.set_index(\"timestamp\")\n",
    "        return self.data\n",
    "\n",
    "class HFDataset(HuggingfaceDataset):\n",
    "    def preprocess(self, return_emojis = False, return_hashtags = False):\n",
    "        ads_keywords = [\"nft\", \"bonus\", \"campaign\", \"invite\", \"friends\"]\n",
    "        def run_all_preprocess_functions(item):\n",
    "            text_list = item['text']\n",
    "            is_ads_tweet = False\n",
    "            emojis_list = []\n",
    "            hashtags_list = []\n",
    "            for enum,text in enumerate(text_list):\n",
    "                text = HFDataset.lowercase_tweet(text)\n",
    "                text = HFDataset.remove_URL(text)\n",
    "                emojis_list += HFDataset.extract_emojis_and_emoticons(text)\n",
    "                hashtags_list += HFDataset.extract_hashtags(text)\n",
    "                text = HFDataset.remove_user_ids(text)\n",
    "                text = HFDataset.remove_punctuations(text)\n",
    "                text_list[enum] = HFDataset.replace_with_BTC(text)\n",
    "                if (not is_ads_tweet):\n",
    "                    is_ads_tweet = HFDataset.is_ads(text, ads_keywords)\n",
    "            if(return_emojis == False):\n",
    "                if(return_hashtags == False):\n",
    "                    return {\"text\" : text_list}\n",
    "                else:\n",
    "                    return {\"text\" : text_list, \"hashtags\" : hashtags_list}\n",
    "            else:\n",
    "                if(return_hashtags == False):\n",
    "                    return {\"text\" : text_list, \"emojis\" : emojis_list}\n",
    "                else:\n",
    "                    return {\"text\" : text_list, \"emojis\" : emojis_list, \"hashtags\" : hashtags_list}\n",
    "\n",
    "        preprocessed_data =  self.map(run_all_preprocess_functions, batched = True)\n",
    "        return preprocessed_data\n",
    "        # self.tokenizer = tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(tokenizer, dataset):\n",
    "        # Tokenize the text field in the dataset\n",
    "        def tokenize_function(tokenizer, item):\n",
    "            # Tokenize the text and return only the necessary fields\n",
    "            encoded = tokenizer(item[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "            return {\"input_ids\": encoded[\"input_ids\"], \"attention_mask\": encoded[\"attention_mask\"], \"label\": item[\"label\"]}\n",
    "\n",
    "        # tokenizing the dataset text to be used in train and test loops\n",
    "        partial_tokenize_function = partial(tokenize_function, tokenizer)\n",
    "        tokenized_datasets = dataset.map(partial_tokenize_function, batched=True)\n",
    "\n",
    "        return tokenized_datasets\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_URL(text):\n",
    "            return re.sub(r\"(?:https?://|www\\.)\\S+\\.\\S+\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def lowercase_tweet(text):\n",
    "        return text.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuations(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        for char in ['!','?','%','$','&']:\n",
    "            exclude.remove(char)\n",
    "        text_without_punctuations = ''.join(ch for ch in text if ch not in exclude)\n",
    "        return text_without_punctuations\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_hashtags(text):\n",
    "        hashtags_list = re.findall(r\"#(\\w+)\", text)\n",
    "        return hashtags_list\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_with_BTC(text):\n",
    "        return re.sub(r\"Bitcoin|bitcoin|btc|BitCoin\", \"BTC\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_user_ids(text):\n",
    "        return re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emojis_and_emoticons(text):\n",
    "        emojis = emoji.distinct_emoji_list(text)\n",
    "        emoticons = []\n",
    "        patterns = [\n",
    "            r\":\\)+\",  # :) or :-) - Smiling face (happiness, amusement, friendliness)\n",
    "            r\":\\(-\",  # :( or :-( - Frowning face (sadness, disappointment, disapproval)\n",
    "            r\";\\)\",   # ;) or ;-) - Wink (joke, flirtation, secrecy)\n",
    "            r\":D+|:-D+\",  # :D or :-D - Big smile or grin (amusement, laughter, joy)\n",
    "            r\":P+|:-P+\",  # :P or :-P - Sticking out tongue (silliness, teasing, raspberries)\n",
    "            r\":=\\)\",    # Equals sign smile (tentative smile, unsure)\n",
    "            r\"/:\",      # Slash frown (disappointment, annoyance)\n",
    "            r\"\\*-*\",    # Asterisk kiss (hugs and kisses)\n",
    "            r\":\\=\",     # Equals sign sad (grimace, helplessness)\n",
    "            r\":\\\"\",     # Double quote (air quotes, sarcasm)\n",
    "            r\"\\*_*\",   # Asterisk happy face (big smile, eyes closed)\n",
    "            r\"\\(/:\",    # Backslash frown (extreme frustration)\n",
    "            r\"\\||_\",    # Sleeping face (tired, bored)\n",
    "            r\"\\^_^\",    # Happy face with underscore eyes (content, smug)\n",
    "            r\":*-^\",    # Cat face (playful, mischief)\n",
    "            r\"^-^\",    # Simple happy face\n",
    "            r\":*_^\",    # Wink with happy face\n",
    "            r\"^_*\",    # Happy face with wink\n",
    "            r\"^-*\",    # Confused face\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            emoticons.extend(re.findall(pattern, text))\n",
    "        return emojis + emoticons\n",
    "\n",
    "    @staticmethod\n",
    "    def is_ads(text, ads_keywords):\n",
    "        for ads_keyword in ads_keywords:\n",
    "            if ads_keyword in text :\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class TextDataset(torchDS):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['label'])\n",
    "        }\n",
    "\n",
    "def calculate_rsi(close_series, length, threshold=(30, 70)):\n",
    "    delta = close_series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=length).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=length).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    description = pd.Series(index=close_series.index, dtype='object')\n",
    "\n",
    "    # Apply dynamic thresholds if provided\n",
    "    if threshold is not None:\n",
    "        lower_threshold, upper_threshold = threshold\n",
    "        description[rsi > upper_threshold] = 'bearish'\n",
    "        description[rsi < lower_threshold] = 'bullish'\n",
    "        description[(rsi <= upper_threshold) & (rsi >= lower_threshold)] = 'neutral'\n",
    "\n",
    "    return rsi, description\n",
    "\n",
    "def calculate_returns(close_series, length, std_coef = (-0.5, 0.4)):\n",
    "    returns = close_series.pct_change()\n",
    "    neg_std_coef, pos_std_coef = std_coef\n",
    "    rolling_std = returns.rolling(window=length).std()\n",
    "    rolling_neg_std_effect = rolling_std * neg_std_coef\n",
    "    rolling_pos_std_effect = rolling_std * pos_std_coef\n",
    "\n",
    "    # Descriptive values\n",
    "    description = pd.Series(index=close_series.index, dtype='object')\n",
    "    description[returns < rolling_neg_std_effect] = 'bearish'\n",
    "    description[returns > rolling_pos_std_effect] = 'bullish'\n",
    "    description[(returns <= rolling_pos_std_effect) & (returns >= rolling_neg_std_effect)] = 'neutral'\n",
    "\n",
    "    return returns, description\n",
    "\n",
    "def undersample_tweets(df):\n",
    "    # Count the number of tweets for each trend\n",
    "    trend_counts = df['next_day_label'].value_counts()\n",
    "\n",
    "    # Identify the minority class\n",
    "    minority_class = trend_counts.idxmin()\n",
    "    minority_count = trend_counts.min()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the undersampled data\n",
    "    undersampled_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the trends\n",
    "    for trend in df['next_day_label'].unique():\n",
    "        # If this is the minority class, add all tweets to the undersampled data\n",
    "        if trend == minority_class:\n",
    "            undersampled_df = pd.concat([undersampled_df, df[df['next_day_label'] == trend]])\n",
    "        else:\n",
    "            # Otherwise, randomly select a subset of tweets equal to the minority count\n",
    "            subset = df[df['next_day_label'] == trend].sample(minority_count)\n",
    "            undersampled_df = pd.concat([undersampled_df, subset])\n",
    "\n",
    "    return undersampled_df\n",
    "\n",
    "def sample_event_days(data, event_dates, window=1):\n",
    "    \"\"\"\n",
    "    Sample data based on event dates with a specified window around each event.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing the data with a datetime index.\n",
    "    - event_dates (list): List of event dates (as strings or pd.Timestamp).\n",
    "    - window (int): Number of days before and after the event to include in the sample.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Sampled data including the specified window around each event.\n",
    "    \"\"\"\n",
    "    sampled_data = pd.DataFrame()\n",
    "\n",
    "    for event_date in event_dates:\n",
    "        event_date = pd.to_datetime(event_date)\n",
    "        start_date = event_date - pd.Timedelta(days=window)\n",
    "        end_date = event_date + pd.Timedelta(days=window)\n",
    "        sampled_data = pd.concat([sampled_data, data.loc[start_date:end_date]])\n",
    "\n",
    "    return sampled_data.drop_duplicates()\n",
    "\n",
    "def extract_windows(df, max_windows=None):\n",
    "    days = df.groupby(df.index.date).first()\n",
    "    window_origins = days[days['next_day_window_start']].index\n",
    "    windows = []\n",
    "    for i in range(len(window_origins) - 1):\n",
    "        # If max_windows is specified and we've reached the limit, break the loop\n",
    "        if max_windows is not None and len(windows) >= max_windows:\n",
    "            break\n",
    "        # Get the start and end index for each window\n",
    "        start_index = window_origins[i]\n",
    "        end_index = days.index[days.index.get_loc(window_origins[i + 1]) - 1]\n",
    "        # Append the window to the list\n",
    "        windows.append(days.loc[start_index:end_index])\n",
    "    # Append the last window if it doesn't exceed max_windows\n",
    "    if max_windows is None or len(windows) < max_windows:\n",
    "        windows.append(days.loc[window_origins[-1]:])\n",
    "    return windows\n",
    "\n",
    "def extract_tweets(windows, df, max_tweet_packs=None):\n",
    "    extracted_tweets = []\n",
    "    # Add a progress bar for the outer loop\n",
    "    for window in tqdm(windows, desc=\"Processing windows\"):\n",
    "        # Get the dates within the window\n",
    "        dates = window.index\n",
    "        # Initialize a list to store the tweet packs for this window\n",
    "        window_tweet_packs = []\n",
    "        # Find the minimum number of tweets across all days in the window\n",
    "        min_tweet_count = min(\n",
    "            df.loc[date.strftime('%Y-%m-%d'), 'text'].size if isinstance(df.loc[date.strftime('%Y-%m-%d'), 'text'], (pd.Series, pd.DataFrame)) else 1\n",
    "            for date in dates\n",
    "        )\n",
    "        # Limit the number of tweet packs to extract if max_tweet_packs is specified\n",
    "        if max_tweet_packs is not None:\n",
    "            min_tweet_count = min(min_tweet_count, max_tweet_packs)\n",
    "        # Iterate over the range of the minimum tweet count\n",
    "        for i in range(min_tweet_count):\n",
    "            # Initialize a list to store the tweets for this tweet pack\n",
    "            tweet_pack = []\n",
    "            # Iterate over the dates in the window\n",
    "            for date in dates:\n",
    "                # Get the i-th tweet for this date\n",
    "                tweet = df.loc[date.strftime('%Y-%m-%d'), [\"text\", \"next_day_label\"]].iloc[i]\n",
    "                # Add the tweet to the tweet pack\n",
    "                tweet_pack.append(tweet)\n",
    "            # Add the tweet pack to the window tweet packs\n",
    "            window_tweet_packs.append(tweet_pack)\n",
    "        # Add the window tweet packs to the extracted tweets\n",
    "        extracted_tweets.append(window_tweet_packs)\n",
    "    return extracted_tweets\n",
    "\n",
    "def shuffle_tweet_packs(tweet_packs, seed=None):\n",
    "    # If a seed is provided, use it to initialize the random number generator\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    # Make a copy of the tweet packs list\n",
    "    shuffled_packs = tweet_packs.copy()\n",
    "    # Shuffle the copied list in-place\n",
    "    random.shuffle(shuffled_packs)\n",
    "    # Return the shuffled list\n",
    "    return shuffled_packs\n",
    "\n",
    "tweet_packs_to_df = lambda tweet_packs: pd.DataFrame([tweet for pack in tweet_packs for tweet in pack])\n",
    "\n",
    "def generate_tweet_prompts(df, include_previous_label=True, include_roc=True, include_rsi=True):\n",
    "    def id_to_label(x):\n",
    "        return \"bullish\" if x == 2 else \"neutral\" if x == 1 else \"bearish\"\n",
    "\n",
    "    prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt_parts = []\n",
    "        if include_previous_label:\n",
    "            prompt_parts.append(f\"previous label: {id_to_label(row['previous_label'])}\")\n",
    "        if include_roc:\n",
    "            prompt_parts.append(f\"roc: {row['ROC']}\")\n",
    "        if include_rsi:\n",
    "            prompt_parts.append(f\"rsi: {row['RSI']}\")\n",
    "        prompt_parts.append(f\"tweet: {row['text']}\")\n",
    "        prompts.append(\" \".join(prompt_parts))\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def init_neptune_run(name, description, params, tags, notebook_addr = '/kaggle/input/tweet-classification'):\n",
    "    \"\"\"\n",
    "    initializes and returns an instance of neptune run and sends the parameters\n",
    "    \"\"\"\n",
    "    run = neptune.init_run(\n",
    "    # proxies={\n",
    "    #     \"http\": \"http://tracker:nlOv5rC7cL3q3bYR@95.216.41.71:3128\",\n",
    "    #     \"https\": \"http://tracker:nlOv5rC7cL3q3bYR@95.216.41.71:3128\"\n",
    "    # },\n",
    "#     with_id=project_id,\n",
    "    project=\"Financial-NLP/market-aware-embedding\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2YWViODAxNC05MzNkLTRiZGMtOGI4My04M2U3MDViN2U3ODEifQ==\",\n",
    "    name=name,\n",
    "    tags=tags,\n",
    "    description=description\n",
    "    )\n",
    "\n",
    "    run[\"notebook\"].upload(notebook_addr)\n",
    "    run[\"parameters\"] = params\n",
    "    return run\n",
    "\n",
    "def pandas_data_loader(addr: str, columns: List[str], *transforms: Callable[[pd.DataFrame], pd.DataFrame]) -> pd.DataFrame:\n",
    "    # Load the data from the CSV file\n",
    "    df = pd.read_csv(addr, usecols=columns)\n",
    "\n",
    "    # Apply each transform to the DataFrame\n",
    "    for transform in transforms:\n",
    "        df = transform(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Transform index to datetime\n",
    "def index_to_datetime(df, unit=\"s\"):\n",
    "    df.index = pd.to_datetime(df.index, unit=unit)\n",
    "    return df\n",
    "# Transform col to index\n",
    "to_index = lambda col, df: df.set_index(col)\n",
    "# Rename text_plit to text\n",
    "rename = lambda original, new, df: df.rename(columns={original: new})\n",
    "\n",
    "\n",
    "def extract_metrics(data):\n",
    "    records = []\n",
    "    for phase in ['train', 'eval']:\n",
    "        for fold, fold_data in data[phase].items():\n",
    "            for epoch, metrics in fold_data.items():\n",
    "                record = {\n",
    "                    'phase': phase,\n",
    "                    'fold': fold,\n",
    "                    'epoch': epoch,\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1': metrics['f1'],\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'roc_score': metrics['roc_score']\n",
    "                }\n",
    "                records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def add_temporal_context(df):\n",
    "    # Ensure the DataFrame has the required columns\n",
    "    if 'day' not in df.columns or 'text' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'date' and 'text' columns\")\n",
    "\n",
    "    # Add temporal context to the text\n",
    "    temporal_series = df.apply(lambda row: f\"date: {row['day'].strftime('%Y, %B, %d')} {row['text']}\", axis=1)\n",
    "\n",
    "    return temporal_series\n",
    "\n",
    "def optimize_parameters(price_df, param_grid, initial_capital=100000, num_starts=10, optimization_interval='6M'):\n",
    "    if optimization_interval is None:\n",
    "        # Use the whole dataset for optimization\n",
    "        best_params, best_sharpe_ratio, best_net_profit = tune_hyperparameters_scipy(price_df, param_grid, initial_capital, num_starts)\n",
    "        return pd.DataFrame([{\n",
    "            'start': price_df.index[0],\n",
    "            'end': price_df.index[-1],\n",
    "            'params': best_params,\n",
    "            'sharpe_ratio': best_sharpe_ratio,\n",
    "            'net_profit': best_net_profit\n",
    "        }])\n",
    "    else:\n",
    "        # Optimize parameters at specified intervals\n",
    "        intervals = pd.date_range(start=price_df.index[0], end=price_df.index[-1], freq=optimization_interval)\n",
    "        optimized_params = []\n",
    "\n",
    "        for start, end in zip(intervals[:-1], intervals[1:]):\n",
    "            interval_df = price_df.loc[start:end]\n",
    "            best_params, best_sharpe_ratio, best_net_profit = tune_hyperparameters_scipy(interval_df, param_grid, initial_capital, num_starts)\n",
    "            optimized_params.append({\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'params': best_params,\n",
    "                'sharpe_ratio': best_sharpe_ratio,\n",
    "                'net_profit': best_net_profit\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(optimized_params)\n",
    "\n",
    "def tune_hyperparameters_scipy(price_df, param_grid, initial_capital=100000, num_starts=10):\n",
    "    best_sharpe_ratio, best_net_profit, best_params = -np.inf, -np.inf, None\n",
    "\n",
    "    param_space = [\n",
    "        param_grid['volatility_period'],\n",
    "        param_grid['upper_barrier_factor'],\n",
    "        param_grid['lower_barrier_factor'],\n",
    "        param_grid['vertical_barrier'],\n",
    "        param_grid['barrier_type'],\n",
    "        param_grid['touch_type']\n",
    "    ]\n",
    "\n",
    "    bounds = [(0, len(space) - 1) for space in param_space]\n",
    "\n",
    "    def bounds_to_params(x):\n",
    "        return {key: space[int(idx)] for key, space, idx in zip(param_grid.keys(), param_space, x)}\n",
    "\n",
    "    def objective_wrapper(x):\n",
    "        params = bounds_to_params(x)\n",
    "        return sharpe_ratio_objective(params, price_df, initial_capital)\n",
    "\n",
    "    for _ in tqdm(range(num_starts), desc=\"Optimizing\"):\n",
    "        initial_guess = [np.random.randint(len(space)) for space in param_space]\n",
    "        result = minimize(objective_wrapper, initial_guess, method='SLSQP', bounds=bounds)\n",
    "\n",
    "        if result.success and -result.fun > best_sharpe_ratio:\n",
    "            best_sharpe_ratio = -result.fun\n",
    "            best_params = bounds_to_params(result.x)\n",
    "            labeler = TripleBarrierLabeler(**best_params)\n",
    "            labeler.fit(price_df)\n",
    "            labeled_df = labeler.transform()\n",
    "            portfolio_values = buy_and_hold(labeled_df['close'], labeled_df['label'], labeled_df[\"upper_barrier\"], labeled_df[\"lower_barrier\"], initial_capital)\n",
    "            best_net_profit = portfolio_values.iloc[-1] - initial_capital\n",
    "\n",
    "    return best_params, best_sharpe_ratio, best_net_profit\n",
    "\n",
    "def sharpe_ratio_objective(params, price_df, initial_capital):\n",
    "    labeler = TripleBarrierLabeler(**params)\n",
    "    labeler.fit(price_df)\n",
    "    labeled_df = labeler.transform()\n",
    "    labeled_df.set_index(price_df.index, inplace=True)\n",
    "    sharpe_ratio = strategize_and_measure(labeled_df, init_capital=initial_capital)\n",
    "    return -sharpe_ratio  # Minimize negative Sharpe Ratio to maximize Sharpe Ratio\n",
    "\n",
    "def strategize_and_measure(price_df, init_capital=1000, fee=0.006):\n",
    "    entries = price_df[\"label\"] > 1\n",
    "    short_entries = price_df[\"label\"] < 1\n",
    "    # Calculate the percentage changes for TP and SL\n",
    "    price_df['tp_stop'] = (price_df['upper_barrier'] - price_df['close']) / price_df['close']\n",
    "    price_df['sl_stop'] = (price_df['close'] - price_df['lower_barrier']) / price_df['close']\n",
    "    # Replace negative values with 0\n",
    "    price_df['tp_stop'] = price_df['tp_stop'].apply(lambda x: max(x, 0))\n",
    "    price_df['sl_stop'] = price_df['sl_stop'].apply(lambda x: max(x, 0))\n",
    "\n",
    "    # Run the backtest\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=price_df['close'],\n",
    "        entries=entries,\n",
    "        short_entries=short_entries,\n",
    "        # size=size,\n",
    "        tp_stop=price_df['tp_stop'],\n",
    "        sl_stop=price_df['sl_stop'],\n",
    "        init_cash=1000,  # Initial cash value\n",
    "        fees=fee,  # Commission fees (0.5%)\n",
    "        freq=\"D\"\n",
    "    )\n",
    "\n",
    "    return pf.sharpe_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a4cf0",
   "metadata": {},
   "source": [
    "Load TBL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3cbdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>1.035</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.051</td>\n",
       "      <td>101218950.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>1.052</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.096</td>\n",
       "      <td>124526330.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>1.096</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.027</td>\n",
       "      <td>126857788.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>1.027</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.061</td>\n",
       "      <td>98542208.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>1.061</td>\n",
       "      <td>1.139</td>\n",
       "      <td>1.045</td>\n",
       "      <td>1.138</td>\n",
       "      <td>135205051.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open   high    low  close       volume\n",
       "timestamp                                          \n",
       "2022-01-31  1.035  1.060  1.006  1.051  101218950.1\n",
       "2022-02-01  1.052  1.096  1.034  1.096  124526330.3\n",
       "2022-02-02  1.096  1.103  1.022  1.027  126857788.7\n",
       "2022-02-03  1.027  1.062  1.016  1.061   98542208.5\n",
       "2022-02-04  1.061  1.139  1.045  1.138  135205051.6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load price data\n",
    "address = \"optimized_labeled_train.csv\" # @param {\"type\":\"string\",\"placeholder\":\"./raw/daily-2020.csv\"}\n",
    "# Loading the price data\n",
    "columns = [\"timestamp\", \"close\", \"open\", \"high\", \"low\", \"volume\"]\n",
    "price_df = pandas_data_loader(address, columns, partial(to_index, \"timestamp\"), partial(index_to_datetime, unit=None))\n",
    "price_df = price_df.sort_index()\n",
    "# price_df = price_df[\"2019-11-01\":\"2021-01-01\"]\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34e57f",
   "metadata": {},
   "source": [
    "Load twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dadf8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-30 11:36:11+00:00</th>\n",
       "      <td>Lately, I just float around going through the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-30 15:46:48+00:00</th>\n",
       "      <td>If youre betting on youre betting on . Innovat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-29 19:06:43+00:00</th>\n",
       "      <td>IT'S OFFICIAL! MULTIPLAYER MODE INTERACTIVE TR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-29 15:02:23+00:00</th>\n",
       "      <td>Dear Cardano, You asked for innovation, we're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-29 18:49:19+00:00</th>\n",
       "      <td>Picked up a crypto magazine and the first page...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        text\n",
       "created_at                                                                  \n",
       "2022-08-30 11:36:11+00:00  Lately, I just float around going through the ...\n",
       "2022-08-30 15:46:48+00:00  If youre betting on youre betting on . Innovat...\n",
       "2022-08-29 19:06:43+00:00  IT'S OFFICIAL! MULTIPLAYER MODE INTERACTIVE TR...\n",
       "2022-08-29 15:02:23+00:00  Dear Cardano, You asked for innovation, we're ...\n",
       "2022-08-29 18:49:19+00:00  Picked up a crypto magazine and the first page..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load text data\n",
    "sentiment = True # @param {\"type\":\"boolean\",\"placeholder\":\"True\"}\n",
    "address = \"twitter_train.csv\" # @param {\"type\":\"string\",\"placeholder\":\"./raw/labeled_tweets.csv\"}\n",
    "columns = [\"text\", \"created_at\"]\n",
    "text_df = pandas_data_loader(address, columns, partial(to_index, \"created_at\"), partial(index_to_datetime, unit='ns'), partial(rename, \"text_split\", \"text\"))\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531a6ac",
   "metadata": {},
   "source": [
    "Load event data if bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e43ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_BTC:\n",
    "    address = \"bitcoin_events.csv\" # @param {\"type\":\"string\",\"placeholder\":\"./raw/labeled_tweets.csv\"}\n",
    "    columns = [\"title\", \"date\"]\n",
    "    event_df = pandas_data_loader(address, columns, partial(to_index, \"date\"), partial(index_to_datetime, unit='ns'))\n",
    "    event_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9452e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"day\"] = text_df.index\n",
    "text_df[\"day\"] = text_df.day.apply(lambda x: x.date())\n",
    "text_df.index = text_df.index.normalize()\n",
    "text_df = text_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990aa951",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"optimized_labeled_train.csv\"\n",
    "# Loading the price data\n",
    "columns = [\"timestamp\", \"close\", \"open\", \"high\", \"low\", \"volume\", \"label\", \"volatility\", \"window_start\", \"upper_barrier\", \"lower_barrier\"]\n",
    "labeled_df = pandas_data_loader(address, columns, partial(to_index, \"timestamp\"), partial(index_to_datetime, unit=None))\n",
    "labeled_df.rename(inplace=True ,columns={\"label\": \"previous_label\"})\n",
    "labeled_df = labeled_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba108f30",
   "metadata": {},
   "source": [
    "Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07d84dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_BTC:\n",
    "    dates = [date.date() for date in event_df.index]\n",
    "    dates = list(set([index for index in text_df.day if index in dates]))\n",
    "else:\n",
    "    dates = list(set([index for index in text_df.day]))\n",
    "    \n",
    "text_df = text_df[text_df['day'].isin(dates)]\n",
    "text_df = text_df[text_df.index.year != 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47517719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-14 00:00:00+00:00</th>\n",
       "      <td>When you push power to the edges, the edges ne...</td>\n",
       "      <td>2022-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-15 00:00:00+00:00</th>\n",
       "      <td>Wow so many replies. The reason I like is the ...</td>\n",
       "      <td>2022-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-16 00:00:00+00:00</th>\n",
       "      <td>For those who didn't know the origins of the C...</td>\n",
       "      <td>2022-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-16 00:00:00+00:00</th>\n",
       "      <td>Sundaeswap launching on on the 20th of January...</td>\n",
       "      <td>2022-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-16 00:00:00+00:00</th>\n",
       "      <td>You wake up Cardano $ADA is $20 what do you do?</td>\n",
       "      <td>2022-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30 00:00:00+00:00</th>\n",
       "      <td>Traded all my $SOL for $ADA and bought my very...</td>\n",
       "      <td>2022-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30 00:00:00+00:00</th>\n",
       "      <td>What project do you think achieved the most in...</td>\n",
       "      <td>2022-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30 00:00:00+00:00</th>\n",
       "      <td>The year is not over $32 $ADA is still possible!</td>\n",
       "      <td>2022-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30 00:00:00+00:00</th>\n",
       "      <td>Hello Citizens! We have big news for you MICEF...</td>\n",
       "      <td>2022-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30 00:00:00+00:00</th>\n",
       "      <td>TOP Projects by Social Activity 30 December 20...</td>\n",
       "      <td>2022-12-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4168 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        text  \\\n",
       "created_at                                                                     \n",
       "2022-01-14 00:00:00+00:00  When you push power to the edges, the edges ne...   \n",
       "2022-01-15 00:00:00+00:00  Wow so many replies. The reason I like is the ...   \n",
       "2022-01-16 00:00:00+00:00  For those who didn't know the origins of the C...   \n",
       "2022-01-16 00:00:00+00:00  Sundaeswap launching on on the 20th of January...   \n",
       "2022-01-16 00:00:00+00:00    You wake up Cardano $ADA is $20 what do you do?   \n",
       "...                                                                      ...   \n",
       "2022-12-30 00:00:00+00:00  Traded all my $SOL for $ADA and bought my very...   \n",
       "2022-12-30 00:00:00+00:00  What project do you think achieved the most in...   \n",
       "2022-12-30 00:00:00+00:00   The year is not over $32 $ADA is still possible!   \n",
       "2022-12-30 00:00:00+00:00  Hello Citizens! We have big news for you MICEF...   \n",
       "2022-12-30 00:00:00+00:00  TOP Projects by Social Activity 30 December 20...   \n",
       "\n",
       "                                  day  \n",
       "created_at                             \n",
       "2022-01-14 00:00:00+00:00  2022-01-14  \n",
       "2022-01-15 00:00:00+00:00  2022-01-15  \n",
       "2022-01-16 00:00:00+00:00  2022-01-16  \n",
       "2022-01-16 00:00:00+00:00  2022-01-16  \n",
       "2022-01-16 00:00:00+00:00  2022-01-16  \n",
       "...                               ...  \n",
       "2022-12-30 00:00:00+00:00  2022-12-30  \n",
       "2022-12-30 00:00:00+00:00  2022-12-30  \n",
       "2022-12-30 00:00:00+00:00  2022-12-30  \n",
       "2022-12-30 00:00:00+00:00  2022-12-30  \n",
       "2022-12-30 00:00:00+00:00  2022-12-30  \n",
       "\n",
       "[4168 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c17d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI\n",
      "neutral    106\n",
      "bullish     47\n",
      "bearish     22\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROC\n",
       "neutral    59\n",
       "bearish    59\n",
       "bullish    56\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, labeled_df[\"RSI\"] = calculate_rsi(labeled_df.close, 8, (30, 70))\n",
    "_, labeled_df[\"ROC\"] = calculate_returns(labeled_df.close, 8)\n",
    "labeled_df.rename(columns={'label': 'previous_label'}, inplace=True)\n",
    "# Shift the labels such that for each day, the label is set to the next day's label\n",
    "labeled_df[\"next_day_label\"] = labeled_df.previous_label.shift(-1)\n",
    "labeled_df[\"next_day_window_start\"] = labeled_df.window_start.shift(-1)\n",
    "labeled_df.loc[labeled_df.iloc[0].name, 'next_day_window_start'] = True\n",
    "print(labeled_df[\"RSI\"].value_counts())\n",
    "labeled_df[\"ROC\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80abe585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "next_day_label\n",
       "0.0    840\n",
       "2.0    806\n",
       "1.0    394\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.index = text_df.index.tz_localize(None)\n",
    "labeled_df.index = labeled_df.index.tz_localize(None)\n",
    "merged_df = text_df.merge(\n",
    "    labeled_df[[\"next_day_label\", 'next_day_window_start', 'previous_label', 'ROC', \"RSI\"]], left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "merged_df.dropna(inplace=True)\n",
    "merged_df.next_day_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b53958",
   "metadata": {},
   "source": [
    "Sampling and prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee8879a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = undersample_tweets(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aea8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cfc9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"day\"] = balanced_df.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399870a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"prompt_pl_excluded\"] = generate_tweet_prompts(balanced_df, include_previous_label=False)\n",
    "balanced_df[\"prompt_roc_excluded\"] = generate_tweet_prompts(balanced_df, include_rsi=False)\n",
    "balanced_df[\"prompt_rsi_excluded\"] = generate_tweet_prompts(balanced_df, include_roc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "970b27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"prompt\"] = generate_tweet_prompts(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a35c1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"text\"] = merged_df[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "612c8c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3484/3220015607.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  merged_df[\"text\"][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'previous label: neutral roc: neutral rsi: neutral tweet: Because $ADA is vaporware.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c220920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"previous label: bearish roc: neutral rsi: neutral tweet: What's the best community on Cardano? I'll go first, \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify text generation\n",
    "assert \"previous label\" in balanced_df[\"prompt\"].iloc[0], \"Text does not contain 'previous label'\"\n",
    "balanced_df[\"prompt\"].sample(frac=1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6a752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date: 2022, April, 26 previous label: bearish roc: bearish rsi: bullish tweet: I am still trying to understand why  and  dont see the value of $ada. Anyone?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add day prompt to the text (not used for out of sample backtesting comparison)\n",
    "merged_df[\"temporal_prompt\"] = add_temporal_context(merged_df)\n",
    "merged_df[\"temporal_prompt\"].sample(frac=1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ec9b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"text\"] = balanced_df.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a8f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3484/3220015607.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  merged_df[\"text\"][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'previous label: neutral roc: neutral rsi: neutral tweet: Because $ADA is vaporware.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f35b6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = balanced_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5407e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = balanced_df.sort_index()\n",
    "balanced_df[\"label\"] = balanced_df.next_day_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3305b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"label\"] = merged_df.next_day_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f170ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"day\"] = merged_df.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e100411",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = balanced_df.day\n",
    "balanced_df[\"text\"] = balanced_df[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60a31e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32890bd6a6f4bb1908a9422ee4485b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d5006de13049daad7e089d81dbd5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/2040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a3a26559154023b38d24f67d367630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/2040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44db683250694ef290ce0a154e16a0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_ds = HFDataset.from_pandas(balanced_df[[\"text\", \"label\"]])\n",
    "labeled_ds = HFDataset.preprocess(labeled_ds)\n",
    "labeled_ds = labeled_ds.class_encode_column('label')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "labeled_ds = HFDataset.tokenize(\n",
    "    tokenizer, labeled_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77851197",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6370577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"samples\": merged_df.shape[0],\n",
    "    \"SEED\":42,\n",
    "    \"TRAINING_BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 2,\n",
    "    \"LEARNING_RATE\":1e-5,\n",
    "    \"FOLDS\": 5,\n",
    "    \"FROZEN\": 11\n",
    "}\n",
    "results = {\n",
    "    \"params\": params,\n",
    "    \"base\": {f\"fold_{fold + 1}\": {} for fold in range(params.get(\"FOLDS\", 5))},\n",
    "    \"train\": {f\"fold_{fold + 1}\": {f\"epoch_{epoch + 1}\": {} for epoch in range(params.get(\"EPOCHS\", 5))} for fold in range(params.get(\"FOLDS\", 5))},\n",
    "    \"eval\": {f\"fold_{fold + 1}\": {f\"epoch_{epoch + 1}\": {} for epoch in range(params.get(\"EPOCHS\", 5))} for fold in range(params.get(\"FOLDS\", 5))},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8caff7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GroupKFold\n",
    "group_kfold = GroupKFold(n_splits=params.get(\"FOLDS\", 5))\n",
    "train_folds=[]\n",
    "test_folds=[]\n",
    "# Split the dataset into training and evaluation sets\n",
    "for train_index, test_index in group_kfold.split(labeled_ds, labeled_ds['label'], groups):\n",
    "  # Verify no overlaps in days\n",
    "    train_days = set(balanced_df.iloc[train_index][\"day\"])\n",
    "    test_days = set(balanced_df.iloc[test_index][\"day\"])\n",
    "    assert train_days.isdisjoint(test_days), \"There are overlapping days between train and test sets\"\n",
    "    train_folds.append(labeled_ds.select(train_index).shuffle())\n",
    "    test_folds.append(labeled_ds.select(test_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d684bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = labeled_ds.shuffle()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=params.get(\"FOLDS\", 5))\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "for train_index, test_index in kf.split(labeled_ds, labeled_ds[\"label\"]):\n",
    "    train_folds.append(labeled_ds.select(train_index))\n",
    "    test_folds.append(labeled_ds.select(test_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b89be668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b7999fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'previous label bearish roc bullish rsi bullish tweet who said it? were currently in a phase where cardano development is getting ahead of $ada price which i find a much more comfortable position to be in than the times where price got ahead of development'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folds[0][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f70e9233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a519a039f84cc79cf64d29d7a68279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold Progress...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974a93bf87264408bb6b30ef1aa9873c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch Progress...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d263807aa14709a3bbd87ddd420d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /kaggle/input/tweet-classification: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e379d258b41a4ecb8ee336a030366145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e70753ee2844e193a1fcd5a8d135b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_1_1: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eda05956e904955b24ff082b080666e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_1_2: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427aad86ac9445e08e269f89aa13b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch Progress...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9674b3bf50ab4e219ebaddcd9537f5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /kaggle/input/tweet-classification: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ad5cf4c244a1596a78da42ae1a822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2874b9e68c4389be1538684b563755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_2_1: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7252b6ce4bcc41bbad5a83ffb676154d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_2_2: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fd051788104dd99f5c866dcf78e0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch Progress...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b370bac8664f35ab87680f269ec10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /kaggle/input/tweet-classification: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f15fa78aef4be6aad1b3cb28c421d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9f355efa6c4e1a8167c1a51ffc872b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_3_1: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d690b7edb204cdaab79bec236cd77c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_3_2: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc2c0a1d01344d1814e8b99097c3e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch Progress...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e53a06c3ca249a2a6546a87dc0607d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /kaggle/input/tweet-classification: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8431b2f2b24c9abd88de8a4d0c3179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caec887c241477c92e4234c097d94f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_4_1: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc05a4ad9d3744cda5c690cbe3f86b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_4_2: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8de68a75e64bb59f32f2777a81bcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch Progress...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a749100b144c593dbfb5896cfa559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /kaggle/input/tweet-classification: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f772ab0c9d422a87542b7a1f655f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1282538bbde4e2bac3f3abb2e17fee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress...:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_5_1: Path not found or is a not a file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfd57a713754ef69ece83f279e5605f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Progress...:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [error  ] Error occurred during asynchronous operation processing: Cannot upload file /home/vas/Desktop/AlternateMethod1/ADA/model_5_2: Path not found or is a not a file.\n"
     ]
    }
   ],
   "source": [
    "# @title Train and Evaluate\n",
    "exp_name = \"CA\" # @param {\"type\":\"string\",\"placeholder\":\"CUA011\"}\n",
    "for index, _ in tqdm(enumerate(train_folds), desc=\"Fold Progress...\"):\n",
    "    fold_num = index + 1\n",
    "    model = CryptoBERT()\n",
    "    for param in model.model.roberta.encoder.layer[:params.get(\"FROZEN\", 11)].parameters():\n",
    "        param.requires_grad = False\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters(), lr=params[\"LEARNING_RATE\"])\n",
    "    num_epochs = params[\"EPOCHS\"]\n",
    "    num_training_steps = (num_epochs * len(train_folds[0])) // params[\"TRAINING_BATCH_SIZE\"]\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)  # 10% of training steps\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    # Initialize a separate Neptune run for each fold\n",
    "    neptune_run = init_neptune_run(name=f\"{exp_name} - Fold {fold_num}\", description=\"\", params=params, tags=[\"cross-validation\", f\"fold-{fold_num}\"])\n",
    "    for epoch in tqdm(range(params.get(\"EPOCHS\", 3)), desc=\"Epoch Progress...\"):\n",
    "        epoch_num = epoch + 1\n",
    "        fold_epoch_addr = f\"fold_{fold_num}/epoch_{epoch_num}\"\n",
    "        model_name=\"train\"\n",
    "        train_dataset = TextDataset(train_folds[index].shuffle())\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=params[\"TRAINING_BATCH_SIZE\"])\n",
    "        model.model.to(device)\n",
    "        labels, preds, probs, losses = model.train(dataloader=train_dataloader, device=device, optimizer=optimizer, scheduler=scheduler, model_name=f\"train\", neptune_run=neptune_run)\n",
    "        results[model_name][f\"fold_{fold_num}\"][f\"epoch_{epoch_num}\"] = model.compute_metrics_classification(np.concatenate(labels), np.concatenate(preds), np.concatenate(probs))\n",
    "        model.plot_roc_curve(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_roc_curve.png\", np.concatenate(labels), np.concatenate(probs))\n",
    "        model.plot_confusion_matrix(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_matrix.png\", np.concatenate(labels), np.concatenate(preds))\n",
    "        neptune_run[f\"{model_name}/{fold_epoch_addr}/roc_curve\"].upload(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_roc_curve.png\") if neptune_run else None\n",
    "        neptune_run[f\"{model_name}/{fold_epoch_addr}/matrix\"].upload(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_matrix.png\") if neptune_run else None\n",
    "        model.save_model(f\"./model_{fold_num}\")\n",
    "        neptune_run[\"model_checkpoints/context_aware\"].upload(f\"./model_{fold_num}_{epoch_num}\")\n",
    "\n",
    "        model_name=\"eval\"\n",
    "        test_dataset = TextDataset(test_folds[index])\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=params[\"TRAINING_BATCH_SIZE\"])\n",
    "        model.model.to(device)\n",
    "        labels, preds, probs, losses = model.evaluate(dataloader=test_dataloader, device=device, model_name=f\"eval\", neptune_run=neptune_run)\n",
    "        results[model_name][f\"fold_{fold_num}\"][f\"epoch_{epoch_num}\"] = model.compute_metrics_classification(np.concatenate(labels), np.concatenate(preds), np.concatenate(probs))\n",
    "        model.plot_roc_curve(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_roc_curve.png\", np.concatenate(labels), np.concatenate(probs))\n",
    "        model.plot_confusion_matrix(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_matrix.png\", np.concatenate(labels), np.concatenate(preds))\n",
    "        neptune_run[f\"{model_name}/{fold_epoch_addr}/roc_curve\"].upload(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_roc_curve.png\") if neptune_run else None\n",
    "        neptune_run[f\"{model_name}/{fold_epoch_addr}/matrix\"].upload(f\"./result/figure/{exp_name}/{fold_epoch_addr}/{model_name}_matrix.png\") if neptune_run else None\n",
    "\n",
    "extract_metrics(results).to_csv(\"./CA_results.csv\")\n",
    "neptune_run[f\"results\"].upload(f\"./CA_results.csv\")\n",
    "neptune_run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
